{
 "metadata": {
  "name": "",
  "signature": "sha256:a5dcc157953068edbf35bb6f2dc4a54ae8a2b4084fb918df5d50f83490faebac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Goals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For us to learn:\n",
      "\n",
      "* the basics of how to process CommonCrawl data by counting files and tallying file sizes in the CC crawl\n",
      "* **how to do this processing in parallel fashion using PiCloud, Amazon AWS (specifically S3), the [`boto` library](http://boto.readthedocs.org/en/latest/)** \n",
      "\n",
      "This notebook duplicates some of [Day_20_CommonCrawl_Starter](http://nbviewer.ipython.org/urls/raw.github.com/rdhyee/working-open-data/master/notebooks/Day_20_CommonCrawl_Starter.ipynb).\n",
      "\n",
      "For moving files between your computer and PiCloud, look at [Day_20_Moving_files_to_PiCloud.ipynb](http://nbviewer.ipython.org/urls/raw.github.com/rdhyee/working-open-data/master/notebooks/Day_20_Moving_files_to_PiCloud.ipynb).\n",
      "\n",
      "For understanding the actual content of the files in Common Crawl, we'll look at [Day_21_CommonCrawl_Content.ipynb](http://nbviewer.ipython.org/urls/raw.github.com/rdhyee/working-open-data/master/notebooks/Day_21_CommonCrawl_Content.ipynb)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning about Common Crawl structure"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good to review Dave Lester's talk: http://www.slideshare.net/davelester/introduction-to-common-crawl  \n",
      "\n",
      "If you need general intro to Common Crawl, watch the [Common Crawl Video](https://www.youtube.com/watch?v=ozX4GvUWDm4)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Common Crawl data stored in Amazon S3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Common Crawl data structure is documented at https://commoncrawl.atlassian.net/wiki/display/CRWL/About+the+Data+Set. To quote the docs:\n",
      "\n",
      "The entire Common Crawl data set is stored on Amazon S3 as a Public Data Set:\n",
      "\n",
      "    http://aws.amazon.com/datasets/41740\n",
      "\n",
      "The data set is divided into three major subsets:\n",
      "\n",
      "* Archived Crawl #1 - s3://aws-publicdatasets/common-crawl/crawl-001/ - crawl data from 2008/2010\n",
      "* Archived Crawl #2 - s3://aws-publicdatasets/common-crawl/crawl-002/ - crawl data from 2009/2010\n",
      "* Current Crawl - s3://aws-publicdatasets/common-crawl/parse-output/ - crawl data from 2012\n",
      "\n",
      "The two archived crawl data sets are stored in folders organized by the year, month, date, and hour the content was crawled.  For example:\n",
      "\n",
      "    s3://aws-publicdatasets/common-crawl/crawl-002/2010/01/06/10/1262847572760_10.arc.gz\n",
      "\n",
      "The current crawl data set is stored in the \"parse-output\" folder in a similar manner to how Nutch stores archives.  Crawl data is stored in a \"segments\" subfolder, then in a folder that starts with the UNIX timestamp of crawl start time.  For example:\n",
      "\n",
      "    s3://aws-publicdatasets/common-crawl/parse-output/segment/1341690169105/1341826131693_45.arc.gz\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Using s3cmd and boto to confirm the examples from the documentation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# s3cmd installed in custom PiCloud environment -- and maybe in your local environment too\n",
      "\n",
      "# confirm s3://aws-publicdatasets/common-crawl/crawl-002/2010/01/06/10/1262847572760_10.arc.gz\n",
      "# doc for s3cmd: http://s3tools.org/s3cmd\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-002/2010/01/06/10/1262847572760_10.arc.gz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2012-01-05 19:19 100001092   s3://aws-publicdatasets/common-crawl/crawl-002/2010/01/06/10/1262847572760_10.arc.gz\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "EXERCISE:  use s3cmd to confirm existence of `s3://aws-publicdatasets/common-crawl/parse-output/segment/1341690169105/1341826131693_45.arc.gz`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/segment/1341690169105/1341826131693_45.arc.gz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2012-07-09 10:43 100001274   s3://aws-publicdatasets/common-crawl/parse-output/segment/1341690169105/1341826131693_45.arc.gz\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "using s3cmd to look at parse-output and valid_segments.txt in current crawl"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# looking at parse-output itself\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output-test/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output/\r\n",
        "2012-09-04 05:03         0   s3://aws-publicdatasets/common-crawl/parse-output-test_$folder$\r\n",
        "2013-05-14 21:02         0   s3://aws-publicdatasets/common-crawl/parse-output_$folder$\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# looking at what is contained by parse-output \"folder\"\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output/checkpoint_staging/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output/checkpoints/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output/segment/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output/valid_segments2/\r\n",
        "2013-05-14 21:04         0   s3://aws-publicdatasets/common-crawl/parse-output/checkpoint_staging_$folder$\r\n",
        "2012-11-09 00:10         0   s3://aws-publicdatasets/common-crawl/parse-output/checkpoints_$folder$\r\n",
        "2012-09-05 05:13         0   s3://aws-publicdatasets/common-crawl/parse-output/segment_$folder$\r\n",
        "2012-11-09 11:28      2478   s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt\r\n",
        "2012-09-05 05:13         0   s3://aws-publicdatasets/common-crawl/parse-output/valid_segments2_$folder$\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a list of \"valid segments\" in \n",
      "\n",
      "    s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt\n",
      "\n",
      "-- a list of segments that are part of the current crawl.  Let's download it and study it.\n",
      "\n",
      "See [discussion about valid segments](https://groups.google.com/forum/#!msg/common-crawl/QYTmnttZZyo/NPiXvK8ZeiMJ)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2012-11-09 11:28      2478   s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we can download it:\n",
      "\n",
      "!s3cmd get --force s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt -> ./valid_segments.txt  [1 of 1]\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " 2478 of 2478   100% in    0s     5.09 kB/s\r",
        " 2478 of 2478   100% in    0s     5.09 kB/s\r",
        " 2478 of 2478   100% in    0s     5.09 kB/s  done\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head valid_segments.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1346823845675\r\n",
        "1346823846036\r\n",
        "1346823846039\r\n",
        "1346823846110\r\n",
        "1346823846125\r\n",
        "1346823846150\r\n",
        "1346823846176\r\n",
        "1346876860445\r\n",
        "1346876860454\r\n",
        "1346876860467\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "using boto to study parse-output and valid_segments.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://boto.s3.amazonaws.com/s3_tut.html\n",
      "\n",
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "conn = S3Connection()\n",
      "\n",
      "# turns out there is an anonymous mode in boto for public data sets:\n",
      "# https://github.com/keiw/common_crawl_index/commit/ad341d0a41a828f260c9c08419dadff0dac6cf5b#L0R33\n",
      "#conn=S3Connection(anon=True)\n",
      "\n",
      "bucket = conn.get_bucket('aws-publicdatasets')\n",
      "for key in islice(bucket.list(prefix=\"common-crawl/parse-output/\", delimiter=\"/\"),None):\n",
      "    print key.name.encode('utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "common-crawl/parse-output/checkpoint_staging_$folder$\n",
        "common-crawl/parse-output/checkpoints_$folder$\n",
        "common-crawl/parse-output/segment_$folder$\n",
        "common-crawl/parse-output/valid_segments.txt\n",
        "common-crawl/parse-output/valid_segments2_$folder$\n",
        "common-crawl/parse-output/checkpoint_staging/\n",
        "common-crawl/parse-output/checkpoints/\n",
        "common-crawl/parse-output/segment/\n",
        "common-crawl/parse-output/valid_segments2/\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get valid_segments\n",
      "# https://commoncrawl.atlassian.net/wiki/display/CRWL/About+the+Data+Set\n",
      "\n",
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "\n",
      "conn = S3Connection()\n",
      "bucket = conn.get_bucket('aws-publicdatasets')\n",
      "\n",
      "k = bucket.get_key(\"common-crawl/parse-output/valid_segments.txt\")\n",
      "s = k.get_contents_as_string()\n",
      "\n",
      "valid_segments = filter(None, s.split(\"\\n\"))\n",
      "\n",
      "print len(valid_segments), valid_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "177 1346823845675\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# valid_segments are Unix timestamps (in ms) -- confirm current crawl is from 2012\n",
      "\n",
      "import datetime\n",
      "datetime.datetime.fromtimestamp(float(valid_segments[0])/1000.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using boto to compile stats on each valid segment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As of the time of this writing (April 4, 2013), there are 177 valid segments in the current crawl.  Now, it's time to figure out how to write a Python function called `segment_stats` that takes a segment id and an optional `stop` parameter (for the max number of keys to iterate through) of the form\n",
      "\n",
      "    def segment_stats(seg_id, stop=None):\n",
      "        pass\n",
      "        # YOUR EXERCISE TO FILL IN\n",
      "\n",
      "and returns a `dict` with 2 keys:  \n",
      "\n",
      "* `count` holding the number of keys inside the given valid segment\n",
      "* `size` holding the total number of bytes held in the keys\n",
      "\n",
      "broken down by file type (there are 3 major types):\n",
      "\n",
      "* `arg.gz` for the \n",
      "* 'metadata' for the metadata files\n",
      "* 'textData' for the textdata files\n",
      "* 'success' for success files\n",
      "\n",
      "For example:\n",
      "\n",
      "    segment_stats('1346823845675', None)\n",
      "\n",
      "should return:\n",
      "\n",
      "    {\n",
      "     'count': {'arc.gz': 11904, 'metadata': 4377, 'success': 1, 'textData': 4377},\n",
      "     'size': {'arc.gz': 967409519222,\n",
      "          'metadata': 187079951008,\n",
      "          'success': 0,\n",
      "          'textData': 129994977292}\n",
      "    }"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Start by looking at a small subset of keys from valid_segments[0]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since it can take 10-50 seconds or so to retrieve all the keys in a valid segment, it's worth limiting to say first 10 to get a feel for what you can do with a key.  Run the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import islice\n",
      "\n",
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "\n",
      "conn = S3Connection()\n",
      "bucket = conn.get_bucket('aws-publicdatasets')\n",
      "for key in islice(bucket.list(prefix=\"common-crawl/parse-output/segment/1346823845675/\", delimiter=\"/\"),10):\n",
      "    print key.name.encode('utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# WARNING -- this might take a bit of time to run -- run it to see how long it takes you to get all the keys in this\n",
      "# segment.  time depends on where you are running this code\n",
      "\n",
      "%time all_files = list(islice(bucket.list(prefix=\"common-crawl/parse-output/segment/1346823845675/\", delimiter=\"/\"),None))\n",
      "print len(all_files), all_files[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But it's useful now to have `all_files` to hold all the keys under the segment `1346823845675`  Note, for example, you can get the size of the file and the name -- and the type of file (boto.s3.key.Key)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://boto.readthedocs.org/en/latest/ref/s3.html#module-boto.s3.key\n",
      "\n",
      "file0 = all_files[0]\n",
      "type(file0), file0.name, file0.size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "\n",
      "from itertools import islice\n",
      "from pandas import DataFrame\n",
      "\n",
      "conn= S3Connection()\n",
      "bucket = conn.get_bucket('aws-publicdatasets')\n",
      "\n",
      "# you might find this conversion function between DataFrame and a list of a regular dict useful\n",
      "#https://gist.github.com/mikedewar/1486027#comment-804797\n",
      "def df_to_dictlist(df):\n",
      "    return [{k:df.values[i][v] for v,k in enumerate(df.columns)} for i in range(len(df))]\n",
      "\n",
      "def cc_file_type(path):\n",
      "\n",
      "    fname = path.split(\"/\")[-1]\n",
      "    \n",
      "    if fname[-7:] == '.arc.gz':\n",
      "        return 'arc.gz'\n",
      "    elif fname[:9] == 'textData-':\n",
      "        return 'textData'\n",
      "    elif fname[:9] == 'metadata-':\n",
      "        return 'metadata'\n",
      "    elif fname == '_SUCCESS':\n",
      "        return 'success'\n",
      "    else:\n",
      "        return 'other'\n",
      "    \n",
      "# a first pass, using DataFrame.  Might not be so efficient considering we are returning only totals\n",
      "def segment_stats(seg_id, stop=None):\n",
      "    all_files = islice(bucket.list(prefix=\"common-crawl/parse-output/segment/{0}/\".format(seg_id), delimiter=\"/\"),stop)\n",
      "    df = DataFrame([{'size': f.size if hasattr(f, 'size') else 0, 'name':f.name, 'type':cc_file_type(f.name)} for f in all_files])\n",
      "    return {'count': df_to_dictlist(df[['size','type']].groupby('type').count()[['size']].T)[0],\n",
      "            'size': df_to_dictlist(df[['size', 'type']].groupby('type').sum().astype('int64').T)[0]}\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# another version of segment_stats that doesn't use DataFrame; probably easier to comprehend what's going on too -- and possibly\n",
      "# faster\n",
      "\n",
      "def segment_stats2(seg_id, stop=None):\n",
      "    from collections import Counter\n",
      "    file_count = Counter()\n",
      "    byte_count = Counter()\n",
      "    \n",
      "    all_files = islice(bucket.list(prefix=\"common-crawl/parse-output/segment/{0}/\".format(seg_id), delimiter=\"/\"),stop)\n",
      "    for f in all_files:\n",
      "        file_type = cc_file_type(f.name)\n",
      "        file_count.update({file_type: 1})\n",
      "        byte_count.update({file_type: f.size if hasattr(f, 'size') else 0})\n",
      "    \n",
      "    return {'count': dict(file_count),\n",
      "            'size': dict(byte_count)}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Running segment_status locally and on PiCloud"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# recall the first segment -- let's work on that segment\n",
      "valid_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# look at how long it takes to run locally\n",
      "\n",
      "%time segment_stats(valid_segments[0], None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Rewrite to use multyvac instead of picloud"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multyvac\n",
      "jid = multyvac.submit(segment_stats, '1346823845675', _layer='numpy2')\n",
      "jid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job = multyvac.get(jid)\n",
      "job.get_result()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pull up status -- refresh until done\n",
      "job.status"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this will block until job is done or errors out\n",
      "job.wait()\n",
      "#cloud.join(jid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get your result\n",
      "#cloud.result(jid)\n",
      "\n",
      "job.get_result()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get useful info about job\n",
      "# http://docs.multyvac.com/primer_python.html#more-attributes\n",
      "\n",
      "[attr_ for attr_ in dir(job) if not attr_.startswith(\"_\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(job.created_at, job.finished_at, job.runtime, job.cputime_system, job.cputime_user)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get some specific info\n",
      "# cloud.info(jid, info_requested=['created', 'finished', 'runtime', 'cputime'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Writing code to submit a series of jobs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time, datetime\n",
      "datetime.datetime.now().isoformat()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import uuid\n",
      "from itertools import islice\n",
      "\n",
      "import multyvac\n",
      "\n",
      "segments_to_calculate = list(islice(valid_segments,2))\n",
      "job_name = uuid.uuid4().hex\n",
      "\n",
      "segments_to_calculate\n",
      "job_ids = [multyvac.submit(segment_stats, seg_id,\n",
      "                           _name=job_name, _layer='numpy2',\n",
      "                           _tags={'f':'segment_stats', 'args':seg_id}) for seg_id in \\\n",
      "           segments_to_calculate]\n",
      "\n",
      "job_ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's get the results\n",
      "\n",
      "multyvac.list(name=job_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "What I got the first time"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I had to retry 2 jobs\n",
      "\n",
      "* https://www.picloud.com/accounts/jobs/#/?ujid=344 -> read timed out\n",
      "* 375 -> AttributeError: 'Prefix' object has no attribute 'size'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now tally everything noting the retries -- might be worth writing this generally\n",
      "# THIS CODE REFERS SPECIFICALLY TO RAYMOND YEE'S JOBS -- REPLACE WITH YOUR OWN IDS\n",
      "\n",
      "from pandas import DataFrame\n",
      "\n",
      "import cloud\n",
      "from itertools import izip, ifilter, chain, islice\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "valid_segments\n",
      "segment_jids = xrange(319, 496)\n",
      "retries_seg_ids = ['1346876860789', '1350433106986']\n",
      "retries_jids  = xrange(496, 498)\n",
      "\n",
      "tally = list(ifilter(lambda x: x[2] == 'done', \n",
      "             izip(chain(valid_segments, retries_seg_ids), chain(segment_jids, retries_jids), \n",
      "          cloud.status(list(chain(segment_jids, retries_jids))))))\n",
      "\n",
      "result = cloud.result([jid for (seg_id, jid, status) in tally])\n",
      "\n",
      "# http://docs.picloud.com/moduledoc.html#module-cloud\n",
      "\n",
      "jobs_info = cloud.info(list(islice(chain(segment_jids, retries_jids),None)),\n",
      "                 info_requested=['created', 'finished', 'runtime', 'cputime', 'core']\n",
      "                 )\n",
      "\n",
      "started = [{'jid':k, 'time':v['finished'] - datetime.timedelta(seconds=v['runtime']), 'count': 1} for (k,v) in jobs_info.items()]\n",
      "finished = [{'jid':k, 'time':v['finished'], 'count': -1} for (k,v) in jobs_info.items()]\n",
      "\n",
      "df = DataFrame(started + finished)\n",
      "\n",
      "exclude_n = 4\n",
      "\n",
      "plot(df.sort_index(by='time')['time'][:-exclude_n], df.sort_index(by='time')['count'].cumsum()[:-exclude_n])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "file_counter = Counter()\n",
      "byte_counter = Counter()\n",
      "\n",
      "result = cloud.result([jid for (seg_id, jid, status) in tally])\n",
      "\n",
      "for r in result:\n",
      "    file_counter.update(r['count'])\n",
      "    byte_counter.update(r['size'])\n",
      "    \n",
      "file_counter, byte_counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jobs_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "jobs_counter= Counter()\n",
      "[jobs_counter.update(dict([(k, v[k]) for k in ('cputime.system', 'cputime.user', 'runtime')])) for v in jobs_info.values()]\n",
      "jobs_counter\n",
      "\n",
      "#print (jobs_counter['cputime.user'] + jobs_counter['cputime.system']), (jobs_counter['cputime.user'] + jobs_counter['cputime.system'])/3600. * 0.05\n",
      "print jobs_counter['runtime'], (jobs_counter['runtime'])/3600. * 0.05"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# maybe use pickle to serialize results\n",
      "import pickle\n",
      "s = pickle.loads(pickle.dumps(dict(zip([seg_id for (seg_id, jid, status) in tally], result))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "picloud job infos"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://docs.picloud.com/moduledoc.html#module-cloud\n",
      "\n",
      "jobs_info = cloud.info(list(islice(chain(segment_jids, retries_jids),None)),\n",
      "                 info_requested=['created', 'finished', 'runtime', 'cputime']\n",
      "                 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "started = [{'jid':k, 'time':v['finished'] - datetime.timedelta(seconds=v['runtime']), 'count': 1} for (k,v) in jobs_info.items()]\n",
      "finished = [{'jid':k, 'time':v['finished'], 'count': -1} for (k,v) in jobs_info.items()]\n",
      "\n",
      "df = DataFrame(started + finished)\n",
      "\n",
      "exclude_n = 4\n",
      "\n",
      "plot(df.sort_index(by='time')['time'][:-exclude_n], df.sort_index(by='time')['count'].cumsum()[:-exclude_n])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Try to do this better with automated retry and using cloud.iresult"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "run jobs locally using cloud.mp"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://docs.picloud.com/cloud_cloudmp.html \n",
      "\n",
      "USE_LOCAL = False\n",
      "\n",
      "if USE_LOCAL:\n",
      "    CLOUD = cloud.mp\n",
      "else:\n",
      "    CLOUD = cloud\n",
      "\n",
      "# try setting n_tasks to something less than # of all segments to test out code\n",
      "\n",
      "n_tasks = len(valid_segments)\n",
      "\n",
      "jids = CLOUD.map(segment_stats2, valid_segments[:n_tasks],  [None]*n_tasks, _env='Working_with_Open_Data')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CLOUD.status(jids)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jobs_info = CLOUD.info(jids,\n",
      "                 info_requested=['created', 'finished', 'runtime', 'cputime']\n",
      "                 )\n",
      "\n",
      "from collections import Counter\n",
      "jobs_counter= Counter()\n",
      "[jobs_counter.update(dict([(k, v[k]) for k in ('cputime.system', 'cputime.user', 'runtime')])) for v in jobs_info.values()]\n",
      "jobs_counter\n",
      "\n",
      "#print (jobs_counter['cputime.user'] + jobs_counter['cputime.system']), (jobs_counter['cputime.user'] + jobs_counter['cputime.system'])/3600. * 0.05\n",
      "print \"total runtime (s): \", jobs_counter['runtime'], \"estimated cost: \", (jobs_counter['runtime'])/3600. * 0.05"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot # cores running vs time\n",
      "\n",
      "started = [{'jid':k, 'time':v['finished'] - datetime.timedelta(seconds=v['runtime']), 'count': 1} for (k,v) in jobs_info.items()]\n",
      "finished = [{'jid':k, 'time':v['finished'], 'count': -1} for (k,v) in jobs_info.items()]\n",
      "\n",
      "df = DataFrame(started + finished)\n",
      "\n",
      "plot(df.sort_index(by='time')['time'], df.sort_index(by='time')['count'].cumsum())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "byte_counter, file_counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://stackoverflow.com/a/1823101/7782\n",
      "\n",
      "import locale\n",
      "locale.setlocale(locale.LC_ALL, 'en_US')\n",
      "\n",
      "locale.format(\"%d\", byte_counter['arc.gz'],  grouping=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
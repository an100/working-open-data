{
 "metadata": {
  "name": "",
  "signature": "sha256:952d17f2f1f28953b8fd32765385537d0f057f06b0a365e6f0e6d75f7e7f3ce6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "conn = S3Connection(anon=True)\n",
      "bucket = conn.get_bucket('aws-publicdatasets')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a function to help with looking through S3 buckets\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "def keys_in_bucket(bucket, prefix, truncate_path=True, limit=None):\n",
      "    \"\"\"\n",
      "    given a S3 boto bucket, yields the keys in bucket with the prefix\n",
      "    if truncate_path is True, remove prefix in keys\n",
      "    optional limit to number of keys to return\n",
      "    \"\"\"\n",
      "    keys = islice(bucket.list(prefix=prefix, \n",
      "                                   delimiter=\"/\"),\n",
      "                       limit)\n",
      "    for key in keys:\n",
      "        name = key.name.encode(\"UTF-8\") \n",
      "        if truncate_path:\n",
      "            name = name.replace(prefix, \"\", 1)\n",
      "        yield name\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "What are the complete range of data for CC, with a focus on 2012 data?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# earlier?\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                       DIR   s3://aws-publicdatasets/common-crawl/2012/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/blekko/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/crawl-001/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/crawl-002/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/crawl-data/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/crawl-db/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/crawl-intermediate-1365897600000/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/graph-output/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/index2012/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/job-logs/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/mapred-temp/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/meanpath/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/nutch/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output-test/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/parse-output/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/projects/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/stats-output/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/test-job-logs/\r\n",
        "                       DIR   s3://aws-publicdatasets/common-crawl/wikipedia/\r\n",
        "2013-05-01 20:11         0   s3://aws-publicdatasets/common-crawl/blekko_$folder$\r\n",
        "2013-07-05 18:27         0   s3://aws-publicdatasets/common-crawl/crawl-intermediate-1365897600000_$folder$\r\n",
        "2013-06-27 19:11         0   s3://aws-publicdatasets/common-crawl/index2012_$folder$\r\n",
        "2013-04-26 04:58         0   s3://aws-publicdatasets/common-crawl/mapred-temp_$folder$\r\n",
        "2013-10-10 19:38         0   s3://aws-publicdatasets/common-crawl/meanpath_$folder$\r\n",
        "2013-05-16 12:58         0   s3://aws-publicdatasets/common-crawl/nutch_$folder$\r\n",
        "2012-09-04 05:03         0   s3://aws-publicdatasets/common-crawl/parse-output-test_$folder$\r\n",
        "2013-05-14 21:02         0   s3://aws-publicdatasets/common-crawl/parse-output_$folder$\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(keys_in_bucket(bucket, \"common-crawl/\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for the 2012 data that I was working with in 2013.\n",
      "# not a good idea to take the contents for \"common-crawl/parse-output/segment/\" as valid segments\n",
      "# instead -- depend on s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt\n",
      "\n",
      "# comment out so I won't run \n",
      "\n",
      "# for (i,key) in enumerate(islice(keys_in_bucket(bucket, \"common-crawl/parse-output/segment/\"),\n",
      "#                                 10)):\n",
      "#     print (i, key)\n",
      "    \n",
      "#!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = bucket.get_key(\"common-crawl/parse-output/valid_segments.txt\")\n",
      "s = k.get_contents_as_string()\n",
      "\n",
      "valid_segments = filter(None, s.split(\"\\n\"))\n",
      "\n",
      "print (len(valid_segments), valid_segments[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# focus on s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-15/\n",
      "# http://commoncrawl.org/april-2014-crawl-data-available/\n",
      "\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-15/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# April 2014 crawl\n",
      "\n",
      "\n",
      "http://commoncrawl.org/april-2014-crawl-data-available/\n",
      "\n",
      "*    all segments (CC-MAIN-2014-15/segment.paths.gz)\n",
      "*    all WARC files (CC-MAIN-2014-15/warc.paths.gz)\n",
      "*    all WAT files (CC-MAIN-2014-15/wat.paths.gz)\n",
      "*    all WET files (CC-MAIN-2014-15/wet.paths.gz)\n",
      "\n",
      "https://aws-publicdatasets.s3.amazonaws.com/common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(keys_in_bucket(bucket, \"common-crawl/crawl-data/CC-MAIN-2014-15/\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import StringIO\n",
      "\n",
      "def gzip_from_key(bucket, key_name):\n",
      "    k = bucket.get_key(key_name)\n",
      "    f = gzip.GzipFile(fileobj=StringIO.StringIO(k.get_contents_as_string()))\n",
      "    return f\n",
      "\n",
      "def segments_from_gzip(gz):\n",
      "    s = gz.read()\n",
      "    return filter(None, s.split(\"\\n\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's parse segment.paths.gz\n",
      "\n",
      "valid_segments = segments_from_gzip(gzip_from_key(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz\"))\n",
      "valid_segments[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WAT:  metadata files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "all WAT files (CC-MAIN-2014-15/wat.paths.gz)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments = segments_from_gzip(gzip_from_key(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/wat.paths.gz\"))\n",
      "len(wat_segments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://commoncrawl.org/navigating-the-warc-file-format/\n",
      "\n",
      "json file\n",
      "\n",
      "forgot to check how big the file is before downloading it.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = bucket.get_key(wat_segments[0])\n",
      "k.size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_key_sizes(bucket, keys):\n",
      "    sizes = []\n",
      "    for key in keys:\n",
      "        k = bucket.get_key(key)\n",
      "        sizes.append((key, k.size if hasattr(k, 'size') else 0))\n",
      "    return sizes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_key_sizes(bucket, wat_segments[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://stackoverflow.com/questions/2348317/how-to-write-a-pager-for-python-iterators/2350904#2350904        \n",
      "def grouper(iterable, page_size):\n",
      "    page= []\n",
      "    for item in iterable:\n",
      "        page.append( item )\n",
      "        if len(page) == page_size:\n",
      "            yield page\n",
      "            page= []\n",
      "    if len(page) > 0:\n",
      "        yield page\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# feed this calculation to multyvac\n",
      "\n",
      "from itertools import islice\n",
      "import multyvac\n",
      "\n",
      "def submit_jobs_to_calc_wat_file_sizes():\n",
      "    page_size = 100\n",
      "    jids = []\n",
      "    for page in grouper(wat_segments, page_size):\n",
      "        jid = multyvac.submit(get_key_sizes, bucket, page)\n",
      "        jids.append(jid)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# when I did my calc on 2014.07.24, I got job ids 27 to 471\n",
      "set(sorted(jids)) == set(range(27,472))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_statuses(jids):\n",
      "\n",
      "    jobs = [multyvac.get(jid) for jid in jids]\n",
      "    statuses = [job.status for job in jobs]\n",
      "    return statuses"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_results(jids):\n",
      "    for jid in jids:\n",
      "        job = multyvac.get(jid)\n",
      "        yield job.get_result()\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compile_results(jids):\n",
      "\n",
      "    results = []\n",
      "\n",
      "    for (i, page) in enumerate(islice(get_results(jids),None)):\n",
      "        print (i)\n",
      "        for result in page:\n",
      "            results.append(result)\n",
      "            \n",
      "    return results\n",
      "    \n",
      "len(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import Series, DataFrame\n",
      "df = DataFrame(results, columns=['key', 'size'])\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['size'].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as P\n",
      "P.hist(df['size'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# total byte size of all the wat files\n",
      "print (format(sum(df['size']),\",d\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save results\n",
      "import csv\n",
      "with open('CC-MAIN-2014-15.wat.csv', 'wb') as csvfile:\n",
      "    wat_writer = csv.writer(csvfile, delimiter=',',\n",
      "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
      "    wat_writer.writerow(['key', 'size'])\n",
      "    for result in results:\n",
      "        wat_writer.writerow(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head CC-MAIN-2014-15.wat.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#s = g.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Taking apart a WAT file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "key = bucket.get_key(wat_segments[0])\n",
      "url = key.generate_url(expires_in=0, query_auth=False)\n",
      "url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# looks like within any segment ID, we should expect warc, wat, wet buckets\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-15/segments/1397609521512.15/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gzipstream import GzipStreamFile\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from itertools import islice\n",
      "from boto.s3.key import Key\n",
      "from gzipstream import GzipStreamFile\n",
      "import warc\n",
      "import json\n",
      "\n",
      "\n",
      "def test_gzipstream():\n",
      "\n",
      "  output = []\n",
      "    \n",
      "  # Let's use a random gzipped web archive (WARC) file from the 2014-15 Common Crawl dataset\n",
      "  ## Connect to Amazon S3 using anonymous credentials\n",
      "  conn = boto.connect_s3(anon=True)\n",
      "  pds = conn.get_bucket('aws-publicdatasets')\n",
      "  ## Start a connection to one of the WARC files\n",
      "  k = Key(pds)\n",
      "  k.key = 'common-crawl/crawl-data/CC-MAIN-2014-15/segments/1397609521512.15/warc/CC-MAIN-20140416005201-00000-ip-10-147-4-33.ec2.internal.warc.gz'\n",
      "\n",
      "  # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "  f = warc.WARCFile(fileobj=GzipStreamFile(k))\n",
      "  for num, record in islice(enumerate(f),100):\n",
      "    if record['WARC-Type'] == 'response':\n",
      "      # Imagine we're interested in the URL, the length of content, and any Content-Type strings in there\n",
      "      output.append((record['WARC-Target-URI'], record['Content-Length']))\n",
      "      output.append( '\\n'.join(x for x in record.payload.read().replace('\\r', '').split('\\n\\n')[0].split('\\n') if 'content-type:' in x.lower()))\n",
      "      output.append( '=-=-' * 10)\n",
      " \n",
      "  return output\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def warc_records(key_name, limit=None):\n",
      "    conn = boto.connect_s3(anon=True)\n",
      "    bucket = conn.get_bucket('aws-publicdatasets')\n",
      "    key = bucket.get_key(key_name)\n",
      "\n",
      "    # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "    f = warc.WARCFile(fileobj=GzipStreamFile(key))\n",
      "    for record in islice(f, limit):\n",
      "        yield record"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's compute some stats on the headers\n",
      "from collections import Counter\n",
      "c=Counter()\n",
      "[c.update(record.header.keys()) for record in warc_records(wat_segments[0],100)]\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# warc-type\n",
      "\n",
      "# looks like \n",
      "# first record is 'warcinfo\n",
      "# rest is 'metadata'\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['warc-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# content-type\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['content-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wrecords = warc_records(wat_segments[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record = wrecords.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://warc.readthedocs.org/en/latest/#working-with-warc-header\n",
      "\n",
      "record.header.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#payload\n",
      "dir(record.payload)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record.header.get('content-type')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# payload\n",
      "\n",
      "\n",
      "s = record.payload.read()\n",
      "len(s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if record.header.get('content-type') == 'application/json':\n",
      "    payload = json.loads(s)\n",
      "else:\n",
      "    payload = s\n",
      "    \n",
      "payload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "payload['Envelope']['WARC-Header-Metadata']['WARC-Target-URI']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "urlparse.urlparse(payload['Envelope']['WARC-Header-Metadata']['WARC-Target-URI']).netloc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "\n",
      "def netloc_count_for_segment(segment_id, limit=None):\n",
      "    \n",
      "    c = Counter()\n",
      "\n",
      "    for record in warc_records(segment_id,limit):\n",
      "\n",
      "        s = record.payload.read()\n",
      "        if record.header.get('content-type') == 'application/json':\n",
      "            payload = json.loads(s)\n",
      "            url = payload['Envelope']['WARC-Header-Metadata'].get('WARC-Target-URI')\n",
      "            if url:\n",
      "                netloc = urlparse.urlparse(url).netloc\n",
      "                c.update([netloc])\n",
      "            else:\n",
      "                c.update([None])\n",
      "\n",
      "    return c\n",
      "\n",
      "\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time netloc_count_for_segment(wat_segments[0],100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "apply(netloc_count_for_segment, [wat_segments[0],100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add(x, y):\n",
      "    return x  + y\n",
      "\n",
      "import multyvac\n",
      "jid = multyvac.submit(add, 2, 3 ,_core='c1', _name=\"just add: 2, 3\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job = multyvac.get(jid)\n",
      "job.status"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job.get_result()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multyvac\n",
      "jid = multyvac.submit(netloc_count_for_segment, wat_segments[0],1500,_core='c1', _name=\"wat_seg:0, 1500, c1; new gzipstream\")\n",
      "jid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job = multyvac.get(jid)\n",
      "job"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job.status"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job.get_result()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WARC files:  the final frontier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is it possible to work with pieces of the new warc files without having to download the whole file?  \n",
      "\n",
      "In the 2012 crawl data, there was an index built http://urlsearch.commoncrawl.org/ --> from which you can get a reference to an offset and length inside of the arc.gz file.  With S3, you don't need to then download the whole file but just a chunk....and that chunk itself is unpackable as a gzip file.  (nice feature of gzip?):\n",
      "\n",
      "http://nbviewer.ipython.org/github/rdhyee/working-open-data/blob/postscript/notebooks/Day_21_CommonCrawl_Content.ipynb#Grabbing-pieces-of-the-.arc.gz-files\n",
      "\n",
      "Is there a similar way of working with the 2014 crawl data?  That is, a way of reading off chunks of the warc file without grabbing entire warc files?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}